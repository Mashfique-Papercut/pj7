import numpy as np
import matplotlib.pyplot as plt
from mlxtend.data import loadlocal_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import Accuracy

# Step 1: Load the MNIST Data using mlxtend
train_images, train_labels = loadlocal_mnist(
    images_path='train-images.idx3-ubyte', 
    labels_path='train-labels.idx1-ubyte'
)

test_images, test_labels = loadlocal_mnist(
    images_path='t10k-images.idx3-ubyte', 
    labels_path='t10k-labels.idx1-ubyte'
)

# Step 2: Preprocess the Data
# Normalize the images to [0, 1]
train_images = train_images / 255.0
test_images = test_images / 255.0

# Reshape images to (28, 28, 1) for Keras (grayscale images)
train_images = train_images.reshape(-1, 28, 28, 1)
test_images = test_images.reshape(-1, 28, 28, 1)

# Convert labels to one-hot encoding
train_labels_one_hot = np.eye(10)[train_labels]
test_labels_one_hot = np.eye(10)[test_labels]

# Step 3: Build the CNN Model
model = Sequential()

# First convolutional layer: 32 filters, kernel size 3x3, ReLU activation
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='valid'))

# Max pooling layer: 2x2, stride 2
model.add(MaxPooling2D(pool_size=(2, 2), strides=2))

# Second convolutional layer: 32 filters, kernel size 3x3, ReLU activation
model.add(Conv2D(32, (3, 3), activation='relu', padding='valid'))

# Max pooling layer: 2x2, stride 2
model.add(MaxPooling2D(pool_size=(2, 2), strides=2))

# Flatten the output for the fully connected layers
model.add(Flatten())

# Fully connected layer: 100 neurons, sigmoid activation
model.add(Dense(100, activation='sigmoid'))

# Fully connected layer: 100 neurons, sigmoid activation
model.add(Dense(100, activation='sigmoid'))

# Output layer: 10 neurons (one per class), sigmoid activation
model.add(Dense(10, activation='sigmoid'))

# Step 4: Compile the Model
model.compile(
    loss=MeanSquaredError(),
    optimizer=SGD(learning_rate=10),
    metrics=[Accuracy()]
)

# Step 5: Train the Model with Batch Size 128
model.fit(
    train_images, 
    train_labels_one_hot, 
    epochs=5, 
    batch_size=128
)

# Step 6: Evaluate the Model on Test Data with Batch Size 128
test_loss, test_acc = model.evaluate(test_images, test_labels_one_hot)
print(f"Test accuracy with batch size 128: {test_acc * 100:.2f}%")

# Step 7: Train the Model with Batch Size 32
model.fit(
    train_images, 
    train_labels_one_hot, 
    epochs=5, 
    batch_size=32
)

# Step 8: Evaluate the Model on Test Data with Batch Size 32
test_loss, test_acc = model.evaluate(test_images, test_labels_one_hot)
print(f"Test accuracy with batch size 32: {test_acc * 100:.2f}%")
